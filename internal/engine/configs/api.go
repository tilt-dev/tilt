package configs

import (
	"context"
	"fmt"
	"time"

	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/util/errors"
	"sigs.k8s.io/controller-runtime/pkg/cache"
	ctrlclient "sigs.k8s.io/controller-runtime/pkg/client"

	"github.com/tilt-dev/tilt/internal/controllers/apicmp"
	"github.com/tilt-dev/tilt/internal/store"
	"github.com/tilt-dev/tilt/internal/tiltfile"
	"github.com/tilt-dev/tilt/pkg/apis"
	"github.com/tilt-dev/tilt/pkg/apis/core/v1alpha1"
)

const LabelOwnerKind = v1alpha1.LabelOwnerKind
const LabelOwnerKindTiltfile = v1alpha1.LabelOwnerKindTiltfile

var ownerSelector = labels.SelectorFromSet(labels.Set{LabelOwnerKind: LabelOwnerKindTiltfile})

type object interface {
	ctrlclient.Object
	GetSpec() interface{}
	GetGroupVersionResource() schema.GroupVersionResource
	NewList() runtime.Object
}

type objectSet map[schema.GroupVersionResource]typedObjectSet
type typedObjectSet map[string]object

// Update all the objects in the apiserver that are owned by the Tiltfile.
//
// Here we have one big API object (the Tiltfile loader) create lots of
// API objects of different types. This is not a common pattern in Kubernetes-land
// (where often each type will only own one or two other types). But it's the best way
// to model how the Tiltfile works.
//
// For that reason, this code is much more generic than owned-object creation should be.
//
// In the future, anything that creates objects based on the Tiltfile (e.g., FileWatch specs,
// LocalServer specs) should go here.
func updateOwnedObjects(ctx context.Context, client ctrlclient.Client, tlr tiltfile.TiltfileLoadResult, mode store.EngineMode) error {
	apiObjects := toAPIObjects(tlr, mode)

	// Retry until the cache has started.
	var retryCount = 0
	var existingObjects objectSet
	var err error
	for {
		existingObjects, err = getExistingAPIObjects(ctx, client)
		if err != nil {
			if _, ok := err.(*cache.ErrCacheNotStarted); ok && retryCount < 5 {
				retryCount++
				time.Sleep(200 * time.Millisecond)
				continue
			}
			return err
		}
		break
	}

	err = updateNewObjects(ctx, client, apiObjects, existingObjects)
	if err != nil {
		return err
	}

	// If the tiltfile loader succeeded, garbage collect any old objects.
	//
	// If the tiltfile loader failed, we want to keep those objects around, in case
	// the tiltfile was only partially evaluated and is missing objects.
	if tlr.Error == nil {
		err := removeOrphanedObjects(ctx, client, apiObjects, existingObjects)
		if err != nil {
			return err
		}
	}
	return nil
}

// Fetch all the existing API objects that were generated from the Tiltfile.
func getExistingAPIObjects(ctx context.Context, client ctrlclient.Client) (objectSet, error) {
	result := objectSet{}

	objs := []object{
		&v1alpha1.KubernetesApply{},
		&v1alpha1.ImageMap{},
		&v1alpha1.FileWatch{},
		&v1alpha1.Cmd{},
	}

	// TODO(nick): Parallelize this?
	for _, obj := range objs {
		list := obj.NewList().(ctrlclient.ObjectList)
		err := client.List(ctx, list, &ctrlclient.ListOptions{LabelSelector: ownerSelector})
		if err != nil {
			return nil, err
		}

		s := typedObjectSet{}
		_ = meta.EachListItem(list, func(obj runtime.Object) error {
			cObj := obj.(object)
			s[cObj.GetName()] = cObj
			return nil
		})
		result[obj.GetGroupVersionResource()] = s
	}

	return result, nil
}

// Pulls out all the API objects generated by the Tiltfile.
func toAPIObjects(tlr tiltfile.TiltfileLoadResult, mode store.EngineMode) objectSet {
	result := objectSet{}
	result[(&v1alpha1.KubernetesApply{}).GetGroupVersionResource()] = toKubernetesApplyObjects(tlr)
	result[(&v1alpha1.ImageMap{}).GetGroupVersionResource()] = toImageMapObjects(tlr)
	result[(&v1alpha1.Cmd{}).GetGroupVersionResource()] = toCmdObjects(tlr)
	result[(&v1alpha1.FileWatch{}).GetGroupVersionResource()] = ToFileWatchObjects(WatchInputs{
		Manifests:     tlr.Manifests,
		ConfigFiles:   tlr.ConfigFiles,
		Tiltignore:    tlr.Tiltignore,
		WatchSettings: tlr.WatchSettings,
		EngineMode:    mode,
	})
	return result
}

// Pulls out all the KubernetesApply objects generated by the Tiltfile.
func toKubernetesApplyObjects(tlr tiltfile.TiltfileLoadResult) typedObjectSet {
	result := typedObjectSet{}
	for _, m := range tlr.Manifests {
		if !m.IsK8s() {
			continue
		}

		kTarget := m.K8sTarget()
		name := m.Name.String()
		ka := &v1alpha1.KubernetesApply{
			ObjectMeta: metav1.ObjectMeta{
				Name: name,
				Labels: map[string]string{
					LabelOwnerKind: LabelOwnerKindTiltfile,
				},
				Annotations: map[string]string{
					v1alpha1.AnnotationManifest: name,
					v1alpha1.AnnotationSpanID:   fmt.Sprintf("kubernetesapply:%s", name),
				},
			},
			Spec: kTarget.KubernetesApplySpec,
		}
		result[name] = ka
	}
	return result
}

// Pulls out all the ImageMap objects generated by the Tiltfile.
func toImageMapObjects(tlr tiltfile.TiltfileLoadResult) typedObjectSet {
	result := typedObjectSet{}

	for _, m := range tlr.Manifests {
		for _, iTarget := range m.ImageTargets {
			if iTarget.IsLiveUpdateOnly {
				continue
			}

			name := apis.SanitizeName(iTarget.ID().Name.String())
			// Note that an ImageMap might be in more than one Manifest, so we
			// can't annotate them to a particular manifest.
			im := &v1alpha1.ImageMap{
				ObjectMeta: metav1.ObjectMeta{
					Name: name,
					Annotations: map[string]string{
						v1alpha1.AnnotationManifest: m.Name.String(),
						v1alpha1.AnnotationSpanID:   fmt.Sprintf("imagemap:%s", name),
					},
					Labels: map[string]string{
						LabelOwnerKind: LabelOwnerKindTiltfile,
					},
				},
				Spec: iTarget.ImageMapSpec,
			}
			result[name] = im
		}
	}
	return result
}

// Pulls out all the Cmd objects generated by the Tiltfile.
func toCmdObjects(tlr tiltfile.TiltfileLoadResult) typedObjectSet {
	result := typedObjectSet{}

	for _, m := range tlr.Manifests {
		if !m.IsLocal() {
			continue
		}
		localTarget := m.LocalTarget()
		cmdSpec := localTarget.UpdateCmdSpec
		if cmdSpec == nil {
			continue
		}

		name := localTarget.UpdateCmdName()
		cmd := &v1alpha1.Cmd{
			ObjectMeta: metav1.ObjectMeta{
				Name: name,
				Annotations: map[string]string{
					v1alpha1.AnnotationManifest: m.Name.String(),
					v1alpha1.AnnotationSpanID:   fmt.Sprintf("cmd:%s", name),
				},
				Labels: map[string]string{
					LabelOwnerKind: LabelOwnerKindTiltfile,
				},
			},
			Spec: *cmdSpec,
		}
		result[name] = cmd
	}
	return result
}

// Reconcile the new API objects against the existing API objects.
func updateNewObjects(ctx context.Context, client ctrlclient.Client, newObjects, oldObjects objectSet) error {
	// TODO(nick): Does it make sense to parallelize the API calls?
	errs := []error{}

	// Upsert the new objects.
	for t, s := range newObjects {
		for name, obj := range s {
			var old object
			oldSet, ok := oldObjects[t]
			if ok {
				old = oldSet[name]
			}

			if old == nil {
				err := client.Create(ctx, obj)
				if err != nil {
					errs = append(errs, fmt.Errorf("create %s/%s: %v", obj.GetGroupVersionResource().Resource, obj.GetName(), err))
				}
				continue
			}

			// Are there other fields here we should check?
			// e.g., once labels are generated from the Tiltfile, it seems
			// we should also update the labels when they change.
			if !apicmp.DeepEqual(old.GetSpec(), obj.GetSpec()) {
				obj.SetResourceVersion(old.GetResourceVersion())
				err := client.Update(ctx, obj)
				if err != nil {
					errs = append(errs, fmt.Errorf("update %s/%s: %v", obj.GetGroupVersionResource().Resource, obj.GetName(), err))
				}
				continue
			}
		}
	}
	return errors.NewAggregate(errs)
}

// Garbage collect API objects that are no longer loaded.
func removeOrphanedObjects(ctx context.Context, client ctrlclient.Client, newObjects, oldObjects objectSet) error {
	// Delete any objects that aren't in the new tiltfile.
	errs := []error{}
	for t, s := range oldObjects {
		for name, obj := range s {
			newSet, ok := newObjects[t]
			if ok {
				_, ok := newSet[name]
				if ok {
					continue
				}
			}

			err := client.Delete(ctx, obj)
			if err != nil {
				errs = append(errs, fmt.Errorf("delete %s/%s: %v", obj.GetGroupVersionResource().Resource, obj.GetName(), err))
			}
		}
	}
	return errors.NewAggregate(errs)
}
